#!/bin/bash -l
#SBATCH -N4 --ntasks-per-node=4 --gpus-per-task=1 --cpus-per-task=32 --exclusive
#-SBATCH --time=6:00:00  -J srHpoA
#SBATCH --time=30:00 
#SBATCH -C gpu -A nstaff_g
#SBATCH -q early_science # for early_science, higher priority
#SBATCH --image=nersc/pytorch:ngc-21.08-v2
#-SBATCH  -x cgpu08 # block sick nodes
#SBATCH --array 1-1
# - - - E N D    O F    SLURM    C O M M A N D S

nprocspn=${SLURM_NTASKS_PER_NODE}
#nprocspn=1  # special case for partial use of full node

design=hpoa_50eaf423; epochs=2000
doHPO=0  # on/off switch

N=${SLURM_NNODES}
G=$[ $N * $nprocspn ]
export MASTER_ADDR=`hostname`

arrIdx=${SLURM_ARRAY_TASK_ID}
jobId=${SLURM_ARRAY_JOB_ID}_${arrIdx}  # must not inclide'/'

echo S: JID=${jobId} MASTER_ADDR=$MASTER_ADDR  MASTER_PORT=${MASTER_PORT}=  G=$G  N=$N nprocspn=$nprocspn
nodeList=$(scontrol show hostname $SLURM_NODELIST)
echo S:node-list $nodeList

# grab some variables from environment - if defined
[[ -z "${SRCOS2D_OTHER_PAR}" ]] && otherParStr="  " || otherParStr=" ${SRCOS2D_OTHER_PAR} "
[[ -z "${SRCOS2D_WRK_SUFIX}" ]] && wrkSufix=$jobId || wrkSufix="${SRCOS2D_WRK_SUFIX}"
env |grep SRCOS2D

if [[  $NERSC_HOST == cori ]]   ; then
    echo "S:on Cori-GPU"
    facility=corigpu
    ENGINE=" shifter "
elif [[  $NERSC_HOST == perlmutter ]]   ; then
    echo "S:on Perlmutter"
    facility=perlmutter
    module unload pytorch
    ENGINE=" shifter "
    # bash -c 'printf "#include <stdio.h>\nint main() {  cudaFree(0);printf(\"cudaFree-done\"); }" > dummy.cu && nvcc -o dummy.exe dummy.cu'
    #  opening and closing a GPU context on each node to reset GPUs
    time srun -n$N -l --ntasks-per-node=1 toolbox/dummy.exe

    #due to the NIC topology NCCL doesnâ€™t automatically use Direct RDMA  which controlls  the NICs for multi-node
    # https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-net-gdr-level-formerly-nccl-ib-gdr-level

    export NCCL_NET_GDR_LEVEL=PHB  #enable Direct RDMA
    #export NCCL_NET_GDR_LEVEL=0 #off Direct RDMA 
fi


if [ $doHPO -gt 0 ]; then
    #....... select design from the list  START,  needs to be a job-array
    designLib=/pscratch/sd/b/balewski/tmp_hpoA/b/
    taskList=${designLib}/hpoSetAb.list
    maxTask=`cat ${taskList} |wc -l`
    echo M:doHPO maxTask=$maxTask arrIdx=$arrIdx taskList=$taskList
    # count from 1 to M
    if [ $arrIdx -gt $maxTask ]; then
        echo "M:arrIdx $arrIdx above maxTask=$maxTask, exit"; exit 0
    fi
    line=`head -n $[ $arrIdx ] $taskList |tail -n 1`
    echo M: arrIdx=$arrIdx line=$line
    design=`echo $line | cut -f3 -d\ `
    localBS=`echo $line | cut -f5 -d\ `
    sampSpeed1G=`echo $line | cut -f10 -d\ `
    #....... select design from the list  END
    # compute number of epochs
    nEpoch=`python3 <<EOF
import numpy as np
totSamp=2700
runTimeMnt=20 #+300
fudge=1+$G/80.  #@16 GPUs: truth 1.1, set 1.2
epochTime=fudge*totSamp/$G/$sampSpeed1G
maxNumEpoch=int(60*runTimeMnt/epochTime)
#print('PYT: speed=%1f, numRank=%d, totSamp=%d, epochTime=%.1f maxNumEpoch=%d'%($sampSpeed1G,$G,totSamp,epochTime,maxNumEpoch))	
print(maxNumEpoch)
EOF`
    #echo aa=$nEpoch
    otherParStr=${otherParStr}" --epochs $nEpoch "
    echo HPO otherParStr=${otherParStr}=
fi


baseDir=$SCRATCH/tmp_NyxHydro4kG/
#baseDir=/global/homes/b/balewski/prje/tmp_NyxHydro4kF/
wrkDir=${baseDir}/$wrkSufix
export TORCH_HOME=$baseDir  # for storing VGG model

echo "S: jobId=$SLURM_JOBID  wrkSufix=$wrkSufix  wrkDir=$wrkDir" 
date

export CMD=" python3 -u  train_dist.py    --facility $facility   --design $design  --epochs $epochs --basePath $wrkDir  --expName  $jobId   $otherParStr "

echo S: CMD=$CMD  ENGINE=$ENGINE
codeList="  train_dist.py  predict.py  toolbox/ batchShifter.slr  *.hpar.yaml  "

mkdir -p $wrkDir
cp -rp $codeList  $wrkDir
cd  $wrkDir
echo S:PWD=`pwd`

if [ $doHPO -gt 0 ]; then
    cp -rp ${designLib}/${design}.hpar.yaml .
fi

echo "S:starting   srgan_cosmo2 " `date` " wrkDir= $wrkDir"
time srun -n $G  $ENGINE  toolbox/driveOneTrain.sh  >& log.train

sleep 3

echo S:done train
time srun -n1  $ENGINE   ./predict.py --basePath $baseDir --expName  $jobId  --genSol last  >& log.predict
echo S:done predict
date

#Cancel all my jobs:
#  squeue -u $USER -h | awk '{print $1}' | xargs scancel
