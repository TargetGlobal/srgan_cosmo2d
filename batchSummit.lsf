#!/bin/bash
#BSUB -P AST153  # Peter
#-BSUB -W 00:30
#BSUB -W 2:00
#BSUB -nnodes 30
#BSUB -J lsf-sr
#BSUB -alloc_flags "nvme smt4"
# - - - - - End LSF directives and begin shell commands

#1arrIdx=${LSB_JOBINDEX}
#1jobId=${LSB_JOBID}_${LSB_JOBINDEX}

nprocspn=6  # 6 is correct, 1 works as well for testing

design=dev5e

epochsStr="  "  ; epochsStr=" --epochs 2100 "
#LRfactStr="  " #; LRfactStr=" --LRfactor 1.1 "

#determine number of nodes and total procs
nnodes=$(cat ${LSB_DJOB_HOSTFILE} | sort | uniq | grep -v login | grep -v batch | wc -l)
nprocs=$(( ${nnodes} * ${nprocspn} ))

G=$[ ${nnodes} * ${nprocspn} ]
echo S: job=${LSB_JOBID}  G=$G  N=${nnodes} 

# grab some variables from environment - if dedefined
[[ -z "${SRCOS2D_LR_FACT}" ]] && LRfactStr="  " || LRfactStr=" --LRfactor  ${SRCOS2D_LR_FACT} "
[[ -z "${SRCOS2D_JOBID}" ]] && jobId=$LSB_JOBID || jobId="G${G}_${SRCOS2D_JOBID}"
env |grep SRCOS2D

# load modules
module load open-ce/1.1.3-py38-0

baseDir=/gpfs/alpine/world-shared/ast153/balewski/tmp_NyxHydro4kE/
wrkDir=${baseDir}/${jobId}

echo S:my job  JID=$LSB_JOBID jobId=$jobId
date

#python -V
pwd

export OMP_NUM_THREADS=1
export NCCL_DEBUG=INFO

echo "S: nprocs=$nprocs  nnodes=$nnodes "

export CMD=" python -u   train_dist.py    --facility summit   --design $design --basePath $baseDir  --expName $jobId  $epochsStr $LRfactStr "

echo CMD=$CMD

codeList="  train_dist.py  predict.py  toolbox/ batchShifter.slr  $design.hpar.yaml  "

mkdir -p $wrkDir
cp -rp $codeList  $wrkDir
cd  $wrkDir
echo PWD=`pwd`

# special case: downloading VGG model manually
# see also https://pytorch.org/docs/stable/hub.html
export TORCH_HOME=$baseDir  # for storing VGG model
vggPath=${TORCH_HOME}/hub/checkpoints
if [ ! -f ${vggPath}/vgg19-dcbb9e9d.pth ] ; then 
    echo S: download VGG model to ${vggPath}
    cd ${vggPath}
    pwd
    wget https://download.pytorch.org/models/vgg19-dcbb9e9d.pth
    ls -l
    cd -
else
    echo S: VGG model found at ${vggPath}
fi

export MPLCONFIGDIR=${baseDir}/mpl
#environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.


echo "S:starting  jobId=$jobId srgan_cosmo2 " `date` " wrkDir= $wrkDir"

jsrun -n${nnodes} -a${nprocspn} -c42 -g6 -r1 --smpiargs off  --bind=proportional-packed:7 --launch_distribution=packed stdbuf -o0  toolbox/launch-smt4.sh "$CMD"  >& log.train

sleep 3

echo S:done train
time  ./predict.py --basePath $baseDir --expName $jobId --genSol last  >& log.predict
echo S:done predict
date

echo 'S:done' 


# notes for job array
# bsub -J 'lsf-pitch[11-14]' batchTrainOntra4.lsf
# bsub batchSummit.lsf
# bkill 520324[11]
# bjobs


