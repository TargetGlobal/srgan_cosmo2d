Defaults:
  # note, CFS-based location will run slower vs. SCRATCH
  cfs_out: &CFS_OUT  /global/homes/b/balewski/prje/tmp_NyxHydro4kC/
  cfs_data: &CFS_DATA  /global/homes/b/balewski/prje/data_NyxHydro4k/B/
  summit_data: &AST153_PROJ  /gpfs/alpine/ast153/proj-shared/balewski/superRes-packed_h5/
  summit_out: &AST153_WORLD /gpfs/alpine/world-shared/ast153/balewski/tmp_NyxHydro4kD/

base_path: 
  corigpu:  /global/cscratch1/sd/balewski/tmp_digitalMind/superRes-cgpu/august/
  perlmutter: *CFS_OUT
  summitlogin: *AST153_WORLD
    
data_path:
  #corigpu: /global/cfs/cdirs/m3363/balewski/superRes-packed_h5/
  corigpu:  *CFS_DATA
  perlmutter: /pscratch/sd/b/balewski/superRes-packed_h5/
  #perlmutter: *CFS_DATA
  summit: *AST153_PROJ
  summitlogin: *AST153_PROJ


myId: dev2-manual
batch_size: 6  # V100=8, A100=16
# optional, reduces training samples
#max_glob_samples_per_epoch: 1400  # do 1400 or 256
hr_size: 512
upscale_factor: 4
num_inp_chan: 1  # grey images
#jnum_inp_chan=3  # RGB images

comment: production config for Super-Resolution
const_local_batch: true

model_conf:
  comment: D+G models taken from  Lornatang Liu Changyu, https://github.com/Lornatang/SRGAN-PyTorch
  print_summary: [0, 0]
  tb_add_graph: null

num_cpu_workers: 4
opt_pytorch: {autotune: true, detect_anomaly: false, zerograd: true}

text_log_interval/epochs: 2
checkpoint_interval/epochs: 50
pred_dump_interval/epochs: 100  # 0=OFF 

train_conf:
<<<<<<< HEAD:dev2.hpar.yaml
  D_LR: {decay/epochs: 1300, gamma: 0.4, init: 8.e-5 }
  G_LR: {decay/epochs: 1100, gamma: 0.4, init: 1.e-4}
  adv_warmup_epochs: 100
=======
  D_LR: {decay/epochs: 1300, gamma: 0.5, init: 2.e-4 }
  G_LR: {decay/epochs: 1100, gamma: 0.4, init: 3.e-4}
  adv_warmup_epochs: 50
>>>>>>> 548edbe369f8f5c65defbf7089b7d22747f21b25:dev3.hpar.yaml
  PG_LR: { init: 1.e-4}	

  # Train epochs.
  p_epochs:     100   #  generator training phase.
  epochs:       1000   #  adversarial training phase.

  # Perceptual G-loss function weights:
  pixel_weight: 0.8
  content_weight: 0.2
  advers_weight: 0.03
  fft_weight: 0.04
  
  # legacy variables
  start_p_epoch: 0
  start_epoch:            0    # ???The number of initial iterations of the adversarial training phase. When set to 0, it means incremental training.
  resume:        False  # Set to `True` to continue training from checkpoint
  resume_p_weight:     ""    # Restore the weight of the generator model during generator training.
  resume_d_weight:        ""  
  resume_g_weight:        ""   
