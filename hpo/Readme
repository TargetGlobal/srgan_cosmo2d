Execution of HPO for SRGAN-cosmo2d

A) define start-HPAR which will be partially altered for each hypothesis

B) generate Hpar hypothesis randomly and instantly benchmark it for OOM and speed on 1 GPUs (1 PM node), save only accepted HPar sets and benchmark stats into

outPath=/pscratch/sd/b/balewski/tmp_hpoA/c
  out.../hpar_XX.conf.yaml
  out.../hpar_XX.sum.yaml

B1.) 1-GPU benchmark steps
a) model builds
b) model traines over 2 pre-G and 2 adv epochs

ssh pm
shifter --image=nersc/pytorch:ngc-21.08-v2 ./genHPar_srganCosmo2d.py --hpoName hpoA  --outPath /pscratch/sd/b/balewski/tmp_hpoA/c --numTrials 10

B2.) select proposals you like for full-scale training

/pscratch/sd/b/balewski/tmp_hpoA/a> cat good_* >hpoSetA.list
designLib=/pscratch/sd/b/balewski/tmp_hpoA/a/

C) execute full scale trainining using the above hypothesis
* enable: doHPO=1 in batchShifter.slr
* set correctly designLib=/pscratch/sd/b/balewski/tmp_hpoA/c/
* set  taskList=${designLib}/hpoSetC.list
* adjust time
  for testing at 30 min : runTimeMnt=20 
  for 6h jobs: runTimeMnt=20 +300
  for 4h jobs: runTimeMnt=20 +200

Fire job array
sbatch -a2-2 batchShifter.slr
