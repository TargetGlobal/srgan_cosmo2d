#!/bin/bash -l
#SBATCH -N1 --ntasks-per-node=8 --gpus-per-task=1 --cpus-per-task=8 --exclusive
#SBATCH --time=30:00  -J srCos2d
#SBATCH  -A AST153_crusher
#SBATCH -q batch
#-SBATCH  -x cgpu08 # block sick nodes
# - - - E N D    O F    SLURM    C O M M A N D S
# salloc -A AST153_crusher -p batch  -t 30:00  -N 1 --x11 --ntasks-per-node=8 --gpus-per-task=1 --cpus-per-task=8 --exclusive

nprocspn=${SLURM_NTASKS_PER_NODE}
#nprocspn=1  # special case for partial use of full node

design=dev5e
epochsStr="  " ; epochsStr=" --epochs 3000 "  #6h==>N4:1700, N6:2600, N8:3600
#LRfactStr="  " #; LRfactStr=" --LRfactor 1.1 "

N=${SLURM_NNODES}
G=$[ $N * $nprocspn ]
export MASTER_ADDR=`hostname`

echo S: JID=${SLURM_JOBID} MASTER_ADDR=$MASTER_ADDR  MASTER_PORT=$MASTER_PORT  G=$G  N=$N 
nodeList=$(scontrol show hostname $SLURM_NODELIST)
echo S:node-list $nodeList

# grab some variables from environment - if defined
[[ -z "${SRCOS2D_LR_FACT}" ]] && LRfactStr="  " || LRfactStr=" --LRfactor  ${SRCOS2D_LR_FACT} "
[[ -z "${SRCOS2D_JOBID}" ]] && jobId=$SLURM_JOBID || jobId="${design}_${SRCOS2D_JOBID}"
env |grep SRCOS2D

if [[  $NERSC_HOST == cori ]]   ; then
    echo "S:on Cori-GPU"
    facility=corigpu
elif [[ `hostname -f ` == *crusher.olcf* ]]   ; then
    echo "on Crusher"
    facility=crusher
elif [[  $NERSC_HOST == perlmutter ]]   ; then
    echo "S:on Perlmutter"
    facility=perlmutter
    # bash -c 'printf "#include <stdio.h>\nint main() {  cudaFree(0);printf(\"cudaFree-done\"); }" > dummy.cu && nvcc -o dummy.exe dummy.cu'
    #  opening and closing a GPU context on each node to reset GPUs
    time srun -n$N -l --ntasks-per-node=1 toolbox/dummy.exe

    #due to the NIC topology NCCL doesnâ€™t automatically use Direct RDMA  which controlls  the NICs for multi-node
    # https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-net-gdr-level-formerly-nccl-ib-gdr-level

    export NCCL_NET_GDR_LEVEL=PHB  #on Direct RDMA
fi

#wrkDir0=$SCRATCH/tmp_NyxHydro4kC/
baseDir=/gpfs/alpine/ast153/scratch/balewski/tmp_NyxHydro4kF/
wrkDir=${baseDir}/${jobId}

echo "S: jobId=$jobId  wrkDir=$wrkDir" 
date

export CMD=" python -u   train_dist.py    --facility $facility   --design $design --basePath $baseDir  --expName $jobId  $epochsStr  $LRfactStr "

echo CMD=$CMD

codeList="  train_dist.py  predict.py  toolbox/ batchShifter.slr  $design.hpar.yaml  "

mkdir -p $wrkDir
cp -rp $codeList  $wrkDir
cd  $wrkDir
echo PWD=`pwd`
export MPLCONFIGDIR=$wrkDir
SIMG=/gpfs/alpine/ast153/scratch/balewski/crusher_amd64/rocm-torch_v1.sif

echo "S:starting  jobId=$jobId srgan_cosmo2 " `date` " wrkDir= $wrkDir"
time srun -n $G  singularity exec  $SIMG toolbox/driveOneTrain.sh   >& log.train

sleep 3

echo S:done train
time srun -n1 shifter   ./predict.py --basePath $baseDir --expName $jobId --genSol last  >& log.predict
echo S:done predict
date
